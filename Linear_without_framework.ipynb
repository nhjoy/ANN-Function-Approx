{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and bias\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "\n",
    "    params = {}  # initialize a dictionary for storing parameters\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        params['w'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 # initialize weights\n",
    "        params['b'+str(l)] = np.zeros((layer_dims[l], 1))   # initialize bias\n",
    "\n",
    "        assert(params['w'+str(l)].shape == (layer_dims[l], layer_dims[l-1])) # check the shape of weights\n",
    "        assert(params['b'+str(l)].shape == (layer_dims[l], 1)) # check the shape of bias\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "\n",
    "def forward_pg (A, W, b):\n",
    "    Z = np.dot(W, A) + b # compute the linear combination\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1])) # check the shape of linear combination\n",
    "    cache = (A, W, b) # store the linear combination in cache\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "\n",
    "def activation(A_prev, W, b, activation_type):\n",
    "\n",
    "    if activation_type == \"sigmoid\":\n",
    "        Z, linear_cache = forward_pg(A_prev, W, b) # compute linear combination\n",
    "        A, activation_cache = sigmoid(Z) # compute the sigmoid\n",
    "\n",
    "    elif activation_type == \"relu\":\n",
    "        Z, linear_cache = forward_pg(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    else:\n",
    "        print(\"Wrong activation type!\")\n",
    "        print(\"Please choose from sigmoid or relu!\")\n",
    "        exit(1)\n",
    "    \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1])) # check the shape of activation\n",
    "    cache = (linear_cache, activation_cache) # store the linear and activation combination in cache\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "        A = 1/(1+np.exp(-Z)) # compute the sigmoid\n",
    "        cache = Z\n",
    "\n",
    "        return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)  # compute the ReLU\n",
    "    assert(A.shape == Z.shape) # check the shape of ReLU\n",
    "\n",
    "    cache = Z  # store the ReLU in cache\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters): # compute the forward propagation\n",
    "\n",
    "    caches = [] # initialize a list for storing cache\n",
    "    A = X # initialize the activation A\n",
    "    L = len(parameters) // 2 # number of layers in the network\n",
    "\n",
    "    # compute the activation A for each layer\n",
    "    for l in range(1, L):\n",
    "        A_prev = A # store the previous activation A\n",
    "        A, cache = activation(A_prev, parameters['w'+str(l)], parameters['b'+str(l)], \"relu\") # compute the activation A for the current layer\n",
    "        caches.append(cache) # store the cache in the list\n",
    "\n",
    "    AL, cache = activation(A, parameters['w'+str(L)], parameters['b'+str(L)], \"relu\") # compute the activation A for the last layer\n",
    "    caches.append(cache) # store the cache in the list\n",
    "\n",
    "    assert(AL.shape == (1, X.shape[1])) # check the shape of activation A\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cost function\n",
    "\n",
    "def cost_function(AL, Y):\n",
    "\n",
    "    # Mean squared error cost function\n",
    "    cost = np.sum(np.square(AL - Y)) / Y.shape[1]\n",
    "    assert(cost.shape == ()) # check the shape of cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the parameters using gradient descent\n",
    "\n",
    "def update_params(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters['w'+str(l+1)] = parameters['w'+str(l+1)] - learning_rate * grads['dw'+str(l+1)] # update the weights\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate * grads['db'+str(l+1)] # update the bias\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    # retrieve values from the first cache\n",
    "    A_prev, W, b = cache\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m # compute the gradient of the cost with respect to the weights\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m # compute the gradient of the cost with respect to the bias\n",
    "    dA_prev = np.dot(W.T, dZ) # compute the gradient of the cost with respect to the activation A_prev\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape) \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear activation backward propagation\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_type):\n",
    "\n",
    "    linear_cache, activation_cache = cache # retrieve the cache\n",
    "\n",
    "    if activation_type == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache) # compute the gradient of the cost with respect to the activation A_prev\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache) # compute the gradient of the cost with respect to the activation A_prev\n",
    "\n",
    "    elif activation_type == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache) # compute the gradient of the cost with respect to the activation A_prev\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache) # compute the gradient of the cost with respect to the activation A_prev\n",
    "\n",
    "    else:\n",
    "        print(\"Wrong activation type!\")\n",
    "        print(\"Please choose from sigmoid or relu!\")\n",
    "        exit(1)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation\n",
    "\n",
    "def backward_pg(Al, Y, caches):\n",
    "\n",
    "    grads = {} # initialize a dictionary for storing gradients\n",
    "    L = len(caches) # number of layers in the network\n",
    "    Y = Y.reshape(AL.shape) # reshape Y\n",
    "\n",
    "    dAL = - (np.divide(Y, Al) - np.divide(1 - Y, 1 - Al)) # compute the derivative of cost function with respect to activation A\n",
    "\n",
    "    current_cache = caches[L-1] # get the cache of the last layer\n",
    "    grads[\"dA\"+str(L)], grads[\"dw\"+str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dAL, current_cache, \"relu\") # compute the gradients of the last layer\n",
    "\n",
    "    for l in reversed(range(L-1)): # iterate over the layers in reverse order\n",
    "        current_cache = caches[l] # get the cache of the current layer\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)], current_cache, \"relu\") # compute the gradients of the current layer\n",
    "        grads[\"dA\"+str(l+1)] = dA_prev_temp # store the gradients of the current layer\n",
    "        grads[\"dw\"+str(l+1)] = dW_temp # store the gradients of the current layer\n",
    "        grads[\"db\"+str(l+1)] = db_temp # store the gradients of the current layer\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "layer_dims = [8, 16, 1] # the dimensions of the network\n",
    "\n",
    "def train_model(X, Y, layer_dims, learning_rate = 0.001, num_iter = 500, print_cost = False): # train the model\n",
    "\n",
    "    np.random.seed(1) # initialize the seed for generating random numbers\n",
    "    cost = [] # initialize a list for storing cost\n",
    "\n",
    "    parameters = initialize_parameters(layer_dims) # initialize the parameters\n",
    "\n",
    "    for i in range(num_iter): # iterate for num_iter times\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters) # compute the forward propagation\n",
    "        cost = cost_function(AL, Y) # compute the cost function\n",
    "        grads = backward_pg(AL, Y, caches) # compute the backward propagation\n",
    "        parameters = update_params(parameters, grads, learning_rate) # update the parameters\n",
    "\n",
    "        if print_cost and i % 100 == 0: # print the cost every 100 iterations\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0: # append the\n",
    "            cost.append(cost)\n",
    "        \n",
    "        plt.plot(np.squeeze(cost)) # plot the cost\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n",
    "\n",
    "#Training accuracy\n",
    "\n",
    "# test accuracy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
